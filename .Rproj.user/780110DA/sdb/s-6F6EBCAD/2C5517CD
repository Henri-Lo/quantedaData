{
    "contents" : "\n# with scan\n# profiling shows that using scan is 3x faster than using strsplit\n#' @export\ntokenizeSingle <- function(s, clean=TRUE){\n  if(clean){s <- clean(s)}\n  s <- unlist(s)\n  tokens <- scan(what=\"char\", text=s, quiet=TRUE)\n  return(tokens)\n}\n\n\n#' @export\ntokenize <- function(x, ...) {\n  UseMethod(\"tokenize\")\n}\n\n#' @export\ntokenize.character <- function(text, clean=TRUE, simplify=FALSE){\n  result <- lapply(text, tokenizeSingle, clean=clean)\n  if( simplify | length(result)==1){result <- unlist(result)}\n  return(result)\n}\n\n\n#' @export\ntokenize.corpus <- function(corpus, clean=TRUE){\n  tokens(corpus) <- tokenize(texts(corpus), clean)\n  return(corpus)\n}\n",
    "created" : 1408979734900.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3382367259",
    "id" : "2C5517CD",
    "lastKnownWriteTime" : 1409332351,
    "path" : "~/Dropbox/code/quanteda/R/tokenize.R",
    "project_path" : "R/tokenize.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}